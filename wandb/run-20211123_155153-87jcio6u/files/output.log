------------- Running MASAC --------------
  Params:
    variant: variant_multi_twoarmhandover.json
2021-11-23 15:51:54.193534 CST | Variant:
2021-11-23 15:51:54.194158 CST | {
  "algorithm": "MASAC",
  "algorithm_kwargs": {
    "batch_size": 128,
    "eval_max_path_length": 500,
    "expl_max_path_length": 500,
    "min_num_steps_before_training": 3300,
    "num_epochs": 2000,
    "num_eval_steps_per_epoch": 2500,
    "num_expl_steps_per_train_loop": 2500,
    "num_trains_per_train_loop": 1000
  },
  "eval_environment_kwargs": {
    "control_freq": 20,
    "controller": "OSC_POSE",
    "env_name": "TwoArmHandover",
    "hard_reset": false,
    "horizon": 500,
    "ignore_done": true,
    "reward_scale": 1.0,
    "robots": "['Panda', 'Panda']"
  },
  "expl_environment_kwargs": {
    "control_freq": 20,
    "controller": "OSC_POSE",
    "env_name": "TwoArmHandover",
    "hard_reset": false,
    "horizon": 500,
    "ignore_done": true,
    "reward_scale": 1.0,
    "robots": "['Panda', 'Panda']"
  },
  "policy_kwargs": {
    "hidden_sizes": [
      256,
      256
    ]
  },
  "qf_kwargs": {
    "hidden_sizes": [
      256,
      256
    ]
  },
  "replay_buffer_size": 1000000,
  "seed": 17,
  "trainer_kwargs": {
    "discount": 0.99,
    "policy_lr": 0.001,
    "qf_lr": 0.0005,
    "reward_scale": 1.0,
    "soft_target_tau": 0.005,
    "target_update_period": 5,
    "use_automatic_entropy_tuning": true
  },
  "version": "normal"
}
/home/shenghui/.pyenv/versions/free-mujoco-robosuite/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32
  logger.warn(
/home/shenghui/.pyenv/versions/free-mujoco-robosuite/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([128, 2])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
2021-11-23 15:58:45.984152 CST | [TwoArmHandover_PandaPanda_OSC_POSE_SEED1_2021_11_23_15_51_54_0000--s-0] Epoch 0 finished
----------------------------------  --------------
replay_buffer/size                  5800
trainer/QF1 Loss Agent0               22.4462
trainer/QF1 Loss Agent1               22.5101
trainer/QF2 Loss Agent0               22.513
trainer/QF2 Loss Agent1               22.5011
trainer/Policy Loss                   -4.6941
trainer/Q1 Predictions Agent0 Mean    -0.00261939
trainer/Q1 Predictions Agent0 Std      0.00168578
trainer/Q1 Predictions Agent0 Max      0.00130778
trainer/Q1 Predictions Agent0 Min     -0.00633921
trainer/Q1 Predictions Agent1 Mean    -0.00668257
trainer/Q1 Predictions Agent1 Std      0.00159901
trainer/Q1 Predictions Agent1 Max     -0.00197128
trainer/Q1 Predictions Agent1 Min     -0.0109493
trainer/Q2 Predictions Agent0 Mean    -0.00971128
trainer/Q2 Predictions Agent0 Std      0.00165279
trainer/Q2 Predictions Agent0 Max     -0.00534596
trainer/Q2 Predictions Agent0 Min     -0.0145654
trainer/Q2 Predictions Agent1 Mean    -0.00572312
trainer/Q2 Predictions Agent1 Std      0.00136933
trainer/Q2 Predictions Agent1 Max     -0.0022216
trainer/Q2 Predictions Agent1 Min     -0.00898827
trainer/Q Targets Agent0 Mean          4.69454
trainer/Q Targets Agent0 Std           0.618824
trainer/Q Targets Agent0 Max           7.83335
trainer/Q Targets Agent0 Min           2.99619
trainer/Q Targets Agent1 Mean          4.69728
trainer/Q Targets Agent1 Std           0.618803
trainer/Q Targets Agent1 Max           7.83669
trainer/Q Targets Agent1 Min           3.0031
trainer/Log Pis Mean                  -4.70237
trainer/Log Pis Std                    0.564533
trainer/Log Pis Max                   -3.29021
trainer/Log Pis Min                   -6.18862
trainer/Policy mu Mean                 0.000206039
trainer/Policy mu Std                  0.00107264
trainer/Policy mu Max                  0.00319799
trainer/Policy mu Min                 -0.00225808
trainer/Policy log std Mean            0.000176562
trainer/Policy log std Std             0.00142765
trainer/Policy log std Max             0.00362683
trainer/Policy log std Min            -0.00399354
trainer/Alpha                          0.999
trainer/Alpha Loss                    -0
exploration/num steps total         5800
exploration/num paths total           12
exploration/path length Mean         500
exploration/path length Std            0
exploration/path length Max          500
exploration/path length Min          500
exploration/Rewards Mean               0.0937255
exploration/Rewards Std                0.0124795
exploration/Rewards Max                0.117553
exploration/Rewards Min                0.0561442
exploration/Returns Mean              46.8627
exploration/Returns Std                3.50619
exploration/Returns Max               51.3948
exploration/Returns Min               42.1476
exploration/Actions Mean               0.00742528
exploration/Actions Std                0.628679
exploration/Actions Max                0.999921
exploration/Actions Min               -0.999566
exploration/Num Paths                  5
exploration/Average Returns           46.8627
evaluation/num steps total          2500
evaluation/num paths total             5
evaluation/path length Mean          500
evaluation/path length Std             0
evaluation/path length Max           500
evaluation/path length Min           500
evaluation/Rewards Mean                0.0967221
evaluation/Rewards Std                 0.00357409
evaluation/Rewards Max                 0.10094
evaluation/Rewards Min                 0.0918546
evaluation/Returns Mean               48.361
evaluation/Returns Std                 1.7811
evaluation/Returns Max                50.2672
evaluation/Returns Min                46.1689
evaluation/ExplReturns Mean           48.361
evaluation/ExplReturns Std             1.7811
evaluation/ExplReturns Max            50.2672
evaluation/ExplReturns Min            46.1689
evaluation/Actions Mean                0.000298068
evaluation/Actions Std                 0.00116243
evaluation/Actions Max                 0.00269575
evaluation/Actions Min                -0.00139918
evaluation/Num Paths                   5
evaluation/Average Returns            48.361
time/data storing (s)                  0.0180145
time/evaluation sampling (s)         112.809
time/exploration sampling (s)        114.94
time/logging (s)                       0.0173047
time/saving (s)                        0.00917958
time/training (s)                     24.235
time/epoch (s)                       252.029
time/total (s)                       414.939
Epoch                                  0
----------------------------------  --------------
2021-11-23 16:03:03.213968 CST | [TwoArmHandover_PandaPanda_OSC_POSE_SEED1_2021_11_23_15_51_54_0000--s-0] Epoch 1 finished
----------------------------------  -------------
replay_buffer/size                  8300
trainer/QF1 Loss Agent0                0.0157822
trainer/QF1 Loss Agent1                0.0169334
trainer/QF2 Loss Agent0                0.0178581
trainer/QF2 Loss Agent1                0.0164656
trainer/Policy Loss                   -7.61652
trainer/Q1 Predictions Agent0 Mean     2.86028
trainer/Q1 Predictions Agent0 Std      0.0818995
trainer/Q1 Predictions Agent0 Max      3.08716
trainer/Q1 Predictions Agent0 Min      2.64421
trainer/Q1 Predictions Agent1 Mean     2.85816
trainer/Q1 Predictions Agent1 Std      0.0913874
trainer/Q1 Predictions Agent1 Max      3.10367
trainer/Q1 Predictions Agent1 Min      2.58259
trainer/Q2 Predictions Agent0 Mean     2.85813
trainer/Q2 Predictions Agent0 Std      0.0966445
trainer/Q2 Predictions Agent0 Max      3.10551
trainer/Q2 Predictions Agent0 Min      2.55672
trainer/Q2 Predictions Agent1 Mean     2.86225
trainer/Q2 Predictions Agent1 Std      0.0888749
trainer/Q2 Predictions Agent1 Max      3.08662
trainer/Q2 Predictions Agent1 Min      2.56745
trainer/Q Targets Agent0 Mean          2.88686
trainer/Q Targets Agent0 Std           0.119735
trainer/Q Targets Agent0 Max           3.42606
trainer/Q Targets Agent0 Min           2.61645
trainer/Q Targets Agent1 Mean          2.88975
trainer/Q Targets Agent1 Max           3.42641
trainer/Q Targets Agent1 Min           2.58501
trainer/Log Pis Mean                  -4.76736
trainer/Log Pis Std                    0.27997
trainer/Log Pis Max                   -3.94763
trainer/Log Pis Min                   -5.89906
trainer/Policy mu Mean                -0.0033989
trainer/Policy mu Std                  0.0164891
trainer/Policy mu Max                  0.02289
trainer/Policy mu Min                 -0.0296298
trainer/Policy log std Mean           -0.130868
trainer/Policy log std Std             0.00650389
trainer/Policy log std Max            -0.112848
trainer/Policy log std Min            -0.155794
trainer/Alpha                          0.367467
trainer/Alpha Loss                   -18.7696
exploration/num steps total         8300
exploration/num paths total           17
exploration/path length Mean         500
exploration/path length Std            0
exploration/path length Max          500
exploration/path length Min          500
exploration/Rewards Mean               0.0817847
exploration/Rewards Std                0.0250652
exploration/Rewards Max                0.53777
exploration/Rewards Min                0.0261077
exploration/Returns Mean              40.8924
exploration/Returns Std                5.63572
exploration/Returns Max               45.0477
exploration/Returns Min               29.8285
exploration/Actions Mean              -0.00335707
exploration/Actions Std                0.590867
exploration/Actions Max                0.999355
exploration/Actions Min               -0.999519
exploration/Num Paths                  5
exploration/Average Returns           40.8924
evaluation/num steps total          5000
evaluation/num paths total            10
evaluation/path length Mean          500
evaluation/path length Std             0
evaluation/path length Max           500
evaluation/path length Min           500
evaluation/Rewards Mean                0.0936447
evaluation/Rewards Std                 0.00345389
evaluation/Rewards Max                 0.0987779
evaluation/Rewards Min                 0.0836386
evaluation/Returns Mean               46.8224
evaluation/Returns Std                 0.842827
evaluation/Returns Max                47.592
evaluation/Returns Min                45.2028
evaluation/ExplReturns Mean           46.8224
evaluation/ExplReturns Std             0.842827
evaluation/ExplReturns Max            47.592
evaluation/ExplReturns Min            45.2028
evaluation/Actions Mean               -0.00381263
evaluation/Actions Std                 0.0174516
evaluation/Actions Max                 0.019267
evaluation/Actions Min                -0.026011
evaluation/Num Paths                   5
evaluation/Average Returns            46.8224
time/data storing (s)                  0.0176425
time/evaluation sampling (s)         116.36
time/exploration sampling (s)        114.877
time/logging (s)                       0.0213573
time/saving (s)                        0.0211079
time/training (s)                     25.9286
time/epoch (s)                       257.226
time/total (s)                       672.172
Epoch                                  1
----------------------------------  -------------
2021-11-23 16:06:58.375931 CST | [TwoArmHandover_PandaPanda_OSC_POSE_SEED1_2021_11_23_15_51_54_0000--s-0] Epoch 2 finished
----------------------------------  --------------
replay_buffer/size                  10800
trainer/QF1 Loss Agent0                 0.00243639
trainer/QF1 Loss Agent1                 0.00247437
trainer/QF2 Loss Agent0                 0.00222289
trainer/QF2 Loss Agent1                 0.00211595
trainer/Policy Loss                    -7.47497
trainer/Q1 Predictions Agent0 Mean      2.69197
trainer/Q1 Predictions Agent0 Std       0.0717778
trainer/Q1 Predictions Agent0 Max       2.77353
trainer/Q1 Predictions Agent0 Min       2.27066
trainer/Q1 Predictions Agent1 Mean      2.69271
trainer/Q1 Predictions Agent1 Std       0.0742127
trainer/Q1 Predictions Agent1 Max       2.78638
trainer/Q1 Predictions Agent1 Min       2.23218
trainer/Q2 Predictions Agent0 Mean      2.69008
trainer/Q2 Predictions Agent0 Std       0.0674767
trainer/Q2 Predictions Agent0 Max       2.78592
trainer/Q2 Predictions Agent0 Min       2.36357
trainer/Q2 Predictions Agent1 Mean      2.69579
trainer/Q2 Predictions Agent1 Std       0.0681463
trainer/Q2 Predictions Agent1 Max       2.79967
trainer/Q2 Predictions Agent1 Min       2.33771
trainer/Q Targets Agent0 Mean           2.69288
trainer/Q Targets Agent0 Std            0.0714011
trainer/Q Targets Agent0 Max            2.97633
trainer/Q Targets Agent0 Min            2.40322
trainer/Q Targets Agent1 Mean           2.6972
trainer/Q Targets Agent1 Std            0.0715298
trainer/Q Targets Agent1 Max            2.97981
trainer/Q Targets Agent1 Min            2.38209
trainer/Log Pis Mean                   -4.79037
trainer/Log Pis Std                     0.298757
trainer/Log Pis Max                    -4.16226
trainer/Log Pis Min                    -5.79754
trainer/Policy mu Mean                  0.00152137
trainer/Policy mu Std                   0.00627679
trainer/Policy mu Max                   0.0186961
trainer/Policy mu Min                  -0.0083744
trainer/Policy log std Mean            -0.122304
trainer/Policy log std Std              0.00804966
trainer/Policy log std Max             -0.0993195
trainer/Policy log std Min             -0.137833
trainer/Alpha                           0.135182
trainer/Alpha Loss                    -37.5833
exploration/num steps total         10800
exploration/num paths total            22
exploration/path length Mean          500
exploration/path length Std             0
exploration/path length Max           500
exploration/path length Min           500
exploration/Rewards Mean                0.088549
exploration/Rewards Std                 0.0341417
exploration/Rewards Max                 0.550391
exploration/Rewards Min                 0.0204242
exploration/Returns Mean               44.2745
exploration/Returns Std                10.2478
exploration/Returns Max                55.8228
exploration/Returns Min                31.1193
exploration/Actions Mean                0.00345683
exploration/Actions Std                 0.591404
exploration/Actions Max                 0.998568
exploration/Actions Min                -0.998756
exploration/Num Paths                   5
exploration/Average Returns            44.2745
evaluation/num steps total           7500
evaluation/num paths total             15
evaluation/path length Mean           500
evaluation/path length Std              0
evaluation/path length Max            500
evaluation/path length Min            500
evaluation/Rewards Mean                 0.0919137
evaluation/Rewards Std                  0.00557787
evaluation/Rewards Max                  0.103329
evaluation/Rewards Min                  0.0798962
evaluation/Returns Mean                45.9569
evaluation/Returns Std                  2.45492
evaluation/Returns Max                 49.858
evaluation/Returns Min                 42.738
evaluation/ExplReturns Mean            45.9569
evaluation/ExplReturns Std              2.45492
evaluation/ExplReturns Max             49.858
evaluation/ExplReturns Min             42.738
evaluation/Actions Mean                 0.00177156
evaluation/Actions Std                  0.0054109
evaluation/Actions Max                  0.014746
evaluation/Actions Min                 -0.0037324
evaluation/Num Paths                    5
evaluation/Average Returns             45.9569
time/data storing (s)                   0.0173648
time/evaluation sampling (s)          110.549
time/exploration sampling (s)         101.804
time/logging (s)                        0.0151869
time/saving (s)                         0.00977138
time/training (s)                      22.7551
time/epoch (s)                        235.15
time/total (s)                        907.328
Epoch                                   2
----------------------------------  --------------
2021-11-23 16:10:40.696684 CST | [TwoArmHandover_PandaPanda_OSC_POSE_SEED1_2021_11_23_15_51_54_0000--s-0] Epoch 3 finished
----------------------------------  ---------------
replay_buffer/size                  13300
trainer/QF1 Loss Agent0                 0.000474426
trainer/QF1 Loss Agent1                 0.000605354
trainer/QF2 Loss Agent0                 0.000482962
trainer/QF2 Loss Agent1                 0.000475855
trainer/Policy Loss                    -7.49804
trainer/Q1 Predictions Agent0 Mean      2.70895
trainer/Q1 Predictions Agent0 Std       0.0862385
trainer/Q1 Predictions Agent0 Max       2.84035
trainer/Q1 Predictions Agent0 Min       2.46027
trainer/Q1 Predictions Agent1 Mean      2.7171
trainer/Q1 Predictions Agent1 Std       0.0800986
trainer/Q1 Predictions Agent1 Max       2.82886
trainer/Q1 Predictions Agent1 Min       2.47
trainer/Q2 Predictions Agent0 Mean      2.71199
trainer/Q2 Predictions Agent0 Std       0.0845663
trainer/Q2 Predictions Agent0 Max       2.82203
trainer/Q2 Predictions Agent0 Min       2.45889
trainer/Q2 Predictions Agent1 Mean      2.71495
trainer/Q2 Predictions Agent1 Std       0.0812993
trainer/Q2 Predictions Agent1 Max       2.84593
trainer/Q2 Predictions Agent1 Min       2.47657
trainer/Q Targets Agent0 Mean           2.70718
trainer/Q Targets Agent0 Std            0.0878823
trainer/Q Targets Agent0 Max            2.8659
trainer/Q Targets Agent0 Min            2.4528
trainer/Q Targets Agent1 Mean           2.71148
trainer/Q Targets Agent1 Std            0.0832025
trainer/Q Targets Agent1 Max            2.88498
trainer/Q Targets Agent1 Min            2.4611
trainer/Log Pis Mean                   -4.79
trainer/Log Pis Std                     0.363311
trainer/Log Pis Max                    -3.79046
trainer/Log Pis Min                    -6.14759
trainer/Policy mu Mean                  0.00280998
trainer/Policy mu Std                   0.00688424
trainer/Policy mu Max                   0.0207123
trainer/Policy mu Min                  -0.0136454
trainer/Policy log std Mean            -0.0968726
trainer/Policy log std Std              0.00513254
trainer/Policy log std Max             -0.0812052
trainer/Policy log std Min             -0.112287
trainer/Alpha                           0.0497317
trainer/Alpha Loss                    -56.3721
exploration/num steps total         13300
exploration/num paths total            27
exploration/path length Mean          500
exploration/path length Std             0
exploration/path length Max           500
exploration/path length Min           500
exploration/Rewards Mean                0.0871241
exploration/Rewards Std                 0.0238689
exploration/Rewards Max                 0.545342
exploration/Rewards Min                 0.039746
exploration/Returns Mean               43.562
exploration/Returns Std                 4.74449
exploration/Returns Max                47.8872
exploration/Returns Min                36.0444
exploration/Actions Mean                0.00107785
exploration/Actions Std                 0.600363
exploration/Actions Max                 0.998875
exploration/Actions Min                -0.999621
exploration/Num Paths                   5
exploration/Average Returns            43.562
evaluation/num steps total          10000
evaluation/num paths total             20
evaluation/path length Mean           500
evaluation/path length Std              0
evaluation/path length Max            500
evaluation/path length Min            500
evaluation/Rewards Mean                 0.0896426
evaluation/Rewards Std                  0.00419631
evaluation/Rewards Max                  0.098505
evaluation/Rewards Min                  0.0803045
evaluation/Returns Mean                44.8213
evaluation/Returns Std                  1.33068
evaluation/Returns Max                 46.499
evaluation/Returns Min                 43.6056
evaluation/ExplReturns Mean            44.8213
evaluation/ExplReturns Std              1.33068
evaluation/ExplReturns Max             46.499
evaluation/ExplReturns Min             43.6056
evaluation/Actions Mean                 0.00253764
evaluation/Actions Std                  0.00629911
evaluation/Actions Max                  0.0160169
evaluation/Actions Min                 -0.00676783
evaluation/Num Paths                    5
evaluation/Average Returns             44.8213
time/data storing (s)                   0.0181832
time/evaluation sampling (s)           94.2981
time/exploration sampling (s)         104.359
time/logging (s)                        0.0215932
time/saving (s)                         0.00977897
time/training (s)                      23.6143
time/epoch (s)                        222.321
time/total (s)                       1129.65
Epoch                                   3
----------------------------------  ---------------
2021-11-23 16:14:37.525576 CST | [TwoArmHandover_PandaPanda_OSC_POSE_SEED1_2021_11_23_15_51_54_0000--s-0] Epoch 4 finished
----------------------------------  --------------
replay_buffer/size                  15800
trainer/QF1 Loss Agent0                 0.00125134
trainer/QF1 Loss Agent1                 0.00121484
trainer/QF2 Loss Agent0                 0.00129617
trainer/QF2 Loss Agent1                 0.00121779
trainer/Policy Loss                    -7.4752
trainer/Q1 Predictions Agent0 Mean      2.77142
trainer/Q1 Predictions Agent0 Std       0.0785704
trainer/Q1 Predictions Agent0 Max       2.88287
trainer/Q1 Predictions Agent0 Min       2.35658
trainer/Q1 Predictions Agent1 Mean      2.77555
trainer/Q1 Predictions Agent1 Std       0.0804108
trainer/Q1 Predictions Agent1 Max       2.90457
trainer/Q1 Predictions Agent1 Min       2.34815
trainer/Q2 Predictions Agent0 Mean      2.77606
trainer/Q2 Predictions Agent0 Std       0.0792765
trainer/Q2 Predictions Agent0 Max       2.88723
trainer/Q2 Predictions Agent0 Min       2.33704
trainer/Q2 Predictions Agent1 Mean      2.77563
trainer/Q2 Predictions Agent1 Std       0.0803197
trainer/Q2 Predictions Agent1 Max       2.89831
trainer/Q2 Predictions Agent1 Min       2.33292
trainer/Q Targets Agent0 Mean           2.77333
trainer/Q Targets Agent0 Std            0.0874969
trainer/Q Targets Agent0 Max            3.23928
trainer/Q Targets Agent0 Min            2.4076
trainer/Q Targets Agent1 Mean           2.77857
trainer/Q Targets Agent1 Std            0.0891954
trainer/Q Targets Agent1 Max            3.24911
trainer/Q Targets Agent1 Min            2.39219
trainer/Log Pis Mean                   -4.70496
trainer/Log Pis Std                     0.404151
trainer/Log Pis Max                    -3.60299
trainer/Log Pis Min                    -6.04134
trainer/Policy mu Mean                  0.00694765
trainer/Policy mu Std                   0.0249014
trainer/Policy mu Max                   0.0643096
trainer/Policy mu Min                  -0.0426789
trainer/Policy log std Mean            -0.0735309
trainer/Policy log std Std              0.00769185
trainer/Policy log std Max             -0.0511387
trainer/Policy log std Min             -0.101767
trainer/Alpha                           0.0183036
trainer/Alpha Loss                    -74.8134
exploration/num steps total         15800
exploration/num paths total            32
exploration/path length Mean          500
exploration/path length Std             0
exploration/path length Max           500
exploration/path length Min           500
exploration/Rewards Mean                0.08624
exploration/Rewards Std                 0.019438
exploration/Rewards Max                 0.122381
exploration/Rewards Min                 0.0295081
exploration/Returns Mean               43.12
exploration/Returns Std                 5.65262
exploration/Returns Max                53.5293
exploration/Returns Min                37.7391
exploration/Actions Mean                0.0052897
exploration/Actions Std                 0.608039
exploration/Actions Max                 0.998948
exploration/Actions Min                -0.99929
exploration/Num Paths                   5
exploration/Average Returns            43.12
evaluation/num steps total          12500
evaluation/num paths total             25
evaluation/path length Mean           500
evaluation/path length Std              0
evaluation/path length Max            500
evaluation/path length Min            500
evaluation/Rewards Mean                 0.0685849
evaluation/Rewards Std                  0.0172131
evaluation/Rewards Max                  0.100419
evaluation/Rewards Min                  0.0392087
evaluation/Returns Mean                34.2924
evaluation/Returns Std                  1.65786
evaluation/Returns Max                 35.6283
evaluation/Returns Min                 31.2469
evaluation/ExplReturns Mean            34.2924
evaluation/ExplReturns Std              1.65786
evaluation/ExplReturns Max             35.6283
evaluation/ExplReturns Min             31.2469
evaluation/Actions Mean                 0.00758087
evaluation/Actions Std                  0.0258244
evaluation/Actions Max                  0.0563758
evaluation/Actions Min                 -0.0389787
evaluation/Num Paths                    5
evaluation/Average Returns             34.2924
time/data storing (s)                   0.0228
time/evaluation sampling (s)           95.9528
time/exploration sampling (s)         108.622
time/logging (s)                        0.0187043
time/saving (s)                         0.00991135
time/training (s)                      32.1943
time/epoch (s)                        236.821
time/total (s)                       1366.48
Epoch                                   4
